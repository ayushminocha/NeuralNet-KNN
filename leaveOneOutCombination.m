function [results,outputs,metric] = leaveOneOutCombination(nn_inputs,knn_inputs,targets,options)
% leaveOneOutCombination(nn_inputs,knn_inputs,targets,options)
% nn_inputs: Input data points including training and testing for Neural Network
% knn_inputs: Input data points including training and testing for k-Nearest Neighbour
% targets: Target classes corresponding to the input data points
% options: Struct of options 
%
% Outputs
% results: matrix of targets (column 1) and generated output classes
%          (column 2)
% outputs: vector of the outputs generated by the final NN (between 0 and 1)
% metric: structure containing accuracy, precision, recall, sensitivity,
%         specificity, F-measure and Confusion Matrix

inputlen = size(nn_inputs,1);

accuracy = 0;
results = []; %%Two column, one for target and second for output
%%Use the options from input

nnout = [];
knnout = [];
outputs = [];

for i=1:inputlen    
     
    trainData = nn_inputs([1:i-1,i+1:end],:);
    trainTarget = targets([1:i-1,i+1:end],:);
    testData = nn_inputs(i,:);
    testTarget = targets(i,:);
    
    options.numHiddenLayers = 3;
    options.numNeurons = [size(trainData,2),size(trainData,2),1];

    model = trainMultiLayerPerceptron(trainData, trainTarget, options);
    nn_testOutput = testMultiLayerPerceptron(testData,testTarget,model);      
    
    knnNewFeat = testMultiLayerPerceptron(nn_inputs,targets,model);
    absErr = abs(targets - knnNewFeat);
    %absErr = knnNewFeat;

    temp = knn_inputs;
    tknn_inputs = zeros(size(temp,1),size(temp,2)+1);
    tknn_inputs(:,1:size(temp,2)) = temp;
    tknn_inputs(:,size(temp,2)+1) = absErr;
    
    knn_trainData = tknn_inputs([1:i-1,i+1:end],:);
    knn_trainTarget = targets([1:i-1,i+1:end],:);
    knn_testData = tknn_inputs(i,:);
    knn_testTarget = targets(i,:);
    
    root = buildkdTree(knn_trainData,knn_trainTarget,1);
    knn_testOutput = testkNN(knn_testData,root,options.k,options.L);
    
    temp_knnOutput = testkNN(tknn_inputs,root,options.k,options.L);
    newNNinput = [temp_knnOutput(:,2) knnNewFeat];    
    
    newNN_trainData = newNNinput([1:i-1,i+1:end],:);
    newNN_trainTarget = targets([1:i-1,i+1:end],:);
    newNN_testData = newNNinput(i,:);
    newNN_testTarget = targets(i,:);
    
    options.numNeurons = [size(newNN_trainData,2),size(newNN_trainData,2),1];
    
    finalModel = trainMultiLayerPerceptron(newNN_trainData, newNN_trainTarget, options);
    finalTestOutput = testMultiLayerPerceptron(newNN_testData,newNN_testTarget,finalModel); 
    
    outputs = [outputs;finalTestOutput];
    
    for j=1:size(finalTestOutput,1)
        
        if finalTestOutput(j) > 0.5
            finalTestOutput(j) = 1;
        else
            finalTestOutput(j) = 0;
        end
        
        if testTarget(j) == finalTestOutput(j)
            accuracy = accuracy + 1;
        end
        
    end

   results = [results;testTarget,finalTestOutput];   
    
end

accuracy = accuracy/inputlen;
tempVar1 = sum(results,2);
tempVar2 = results(:,1) - results(:,2);

truePositives = length(find(tempVar1 == 2));
trueNegatives = length(find(tempVar1 == 0));
falsePositives = length(find(tempVar2 == -1));
falseNegatives = length(find(tempVar2 == 1));

sensitivity = truePositives/(truePositives + falseNegatives);
specificity = trueNegatives/(trueNegatives + falsePositives);
precision = truePositives/(truePositives + falsePositives);
recall = truePositives/(truePositives + falseNegatives);

metric = struct;
metric.accuracy = accuracy;
metric.precision = precision;
metric.recall = recall;
metric.sensitivity = sensitivity;
metric.specificity = specificity;
metric.Fmeasure = 2*(precision*recall)/(precision+recall);
metric.ConfMatrix = [truePositives,falseNegatives;falsePositives,trueNegatives];